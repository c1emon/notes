<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>第八章-矩阵特征值问题的解法</title></head>
<body><h1>第八章、矩阵特征值问题的解法</h1>
<h2>一、乘幂法</h2>
<p>假设$A\in R^{n\times n}$（$A$为$n$阶矩阵）的$n$个特征值$\lambda_1,\lambda_2,\cdots,\lambda_n$的相应的<mark>线性无关的特征向量</mark>为$x_1,x_2,\cdots,x_n$。若$x_1,x_2,\cdots,x_n$线性无关，则任意$n$维向量都可以用这一组向量表示，因为这组线性无关的向量是该$n$维空间里的一组基。</p>
<ul>
<li>如果有重特征值，则不一定有线性无关的特征向量。但对于实对称矩阵，一定有$n$个线性无关的特征向量。</li>
<li>重特征值如果为模最大的特征值，也可以使用本节算法。</li>

</ul>
<p>若$A$的特征值满足$|\lambda_1|>|\lambda_2|\geq\cdots\geq|\lambda_n|$，则$Ax_i=\lambda_ix_i$。且对于$\forall{v^{(0)}}\in R^{n\times n}$，有</p>
<div>$$
v^{(0)}=k_1x_1+k_2x_2+\cdots+k_nx_n,\quad (k_1\neq 0)
$$</div>
<p>然后</p>
<div>$$
\begin{aligned}
v^{(1)}&amp;=Av^{(0)}=k_1Ax_1+k_2Ax_2+\cdots+k_nAx_n \\
&amp;=k_1\lambda_1x_1+k_2\lambda_2x_2+\cdots+k_n\lambda_nx_n \\
v^{(2)}&amp;=Av^{(1)}=k_1\lambda_1^2x_1+k_2\lambda_2^2x_2+\cdots+k_n\lambda_n^2x_n \\
v^{(3)}&amp;=Av^{(2)}=\cdots \\
&amp;\quad\vdots \\
v^{(m)}&amp;=Av^{(m-1)}=k_1\lambda_1^mx_1+k_2\lambda_2^mx_2+\cdots+k_n\lambda_n^mx_n \\
&amp;=\lambda_1^m(k_1x_1+k_2\frac{\lambda_2^m}{\lambda_1^m}x_2+\cdots+k_n\frac{\lambda_n^m}{\lambda_1^m}x_n) \\
&amp;=\lambda_1^m(k_1x_1+\sum_{i=2}^n(\frac{\lambda_i}{\lambda_1})^mk_ix_i) \\
v^{(m+1)}&amp;=Av^{(m)}=\lambda_1^{m+1}(k_1x_1+\sum_{i=2}^n(\frac{\lambda_i}{\lambda_1})^{m+1}k_ix_i) \\
&amp;\quad\vdots
\end{aligned}
$$</div>
<p>显然<mark>当m充分大</mark>时，由于$\lambda_1$是最大的特征值，则有</p>
<div>$$
v^{(m)}\approx \lambda_1^mk_1x_1 \\
v^{(m+1)}\approx\lambda_1^{m+1}k_1x_1
$$</div>
<p>即$v^{(m+1)}\approx\lambda_1v^{(m)}$，两边同时对$v^{(m)}$求内积$<v^{(m+1)},v^{(m)}>\approx<\lambda_1v^{(m)},v^{(m)}>$，得</p>
<div>$$
\lambda_1\approx\frac{&lt;v^{(m+1)},v^{(m)}&gt;}{&lt;v^{(m)},v^{(m)}&gt;}
$$</div>
<p>由于$Av^{(m)}\approx\lambda_1v^{(m)}$，$v^{(m)}$就是$\lambda_1$对应的<mark>近似特征向量</mark>。</p>
<p>实际计算中不求内积，而只需要计算第$j$个分量，$v^{(m+1)}$与$v^{(m)}$的第$j$个分量间有如下关系</p>
<div>$$
\frac{(v^{(m+1)})_j}{(v^{(m)})_j}=\lambda_1\frac{(k_1x_1)_j+\sum_{i=2}{n}(\frac{\lambda_i}{\lambda_1})^{m+1}k_i(x_i)_j}{(k_1x_1)_j+\sum_{i=2}^{n}(\frac{\lambda_i}{\lambda_1})^mk_i(x_i)_j}
$$</div>
<p>由于$\lambda_1>\lambda_i,(i=2,3,\cdots,n)$，可得</p>
<div>$$
\lim_{m\to\infty}(\frac{\lambda_i}{\lambda_1})^m=0
$$</div>
<p>则</p>
<div>$$
\lim_{m\to\infty}\frac{(v^{(m+1)})_j}{(v^{(m)})_j}=\lambda_1
$$</div>
<p>取最大的分量</p>
<div>$$
|(v^{(m)})_q|=\max_{1\leq i\leq n}(|(v^{(m)})_i|)
$$</div>
<p>则</p>
<div>$$
\lambda_1\approx\frac{(v^{(m+1)})_q}{(v^{(m)})_q}
$$</div>
<ul>
<li><p>算法：</p>
<ol start='' >
<li><p>取$\forall{v^{(0)}}\neq0$</p>
</li>
<li><p>计算$v^{(m+1)}=Av^{(m)},(m=0,1,2,\cdots,n)$</p>
</li>
<li><p>取最大分量$|(v^{(m)})_q|$</p>
</li>
<li><p>当$m$充分大时</p>
<div>$$
\frac{(v^{(m+1)})_q}{(v^{(m)})_q}\approx C
$$</div>
<p>则取$\lambda_1=C$，而$v^{(m)}$就是$A$按模最大特征值$\lambda_1$对应的一个近似特征向量。</p>
</li>

</ol>
</li>
<li><p>说明：</p>
<ol start='' >
<li><p>若$k_1=0$，则另取其他初始向量$v^{(0)}$</p>
</li>
<li><p>$v^{(m+1)}=\lambda_1^{m+1}(k_1x_1+\sum_{i=2}^n(\frac{\lambda_i}{\lambda_1})^{m+1}k_ix_i)$，</p>
<ul>
<li>若$|\lambda_1|>1$，则$v^{(m+1)}$的分量绝对值可能会很大（溢出）</li>
<li>若$|\lambda_1|<1$，则$v^{(m+1)}$的分量绝对值可能会过小（趋于0）</li>

</ul>
<p>所以在每次迭代时，对$v^{(m+1)}$进行<mark>单位化</mark>，即</p>
<div>$$
(u^{(m)})_i=\frac{(v^{(m)})_i}{(v^{m})_q}
$$</div>
<p>这里在每次迭代时，将每个分量除以最大的分量，归一化。再以新的向量$u^{(m)}$迭代。</p>
</li>
<li><p>控制收敛。实际中采用<mark>前后两次特征值的绝对误差</mark>控制，即</p>
<div>$$
\left|\frac{(v^{(m+1)})_q}{(v^{(m)})_q}-\frac{(v^{(m)})_q}{(v^{(m-1)})_q}\right|\leq\varepsilon
$$</div>
<p>或者判断每次残差$||Av^{(m)}-\lambda_1v^{(m)}||\leq\varepsilon$，即可停止迭代。</p>
</li>
<li><p>$m$充分大时，收敛的速度依赖于$\left|\frac{\lambda_2}{\lambda_1}\right|$的大小。$\left|\frac{\lambda_2}{\lambda_1}\right|$小于1的程度越小，收敛速度越快。</p>
</li>
<li><p>若按模最大的特征值是<mark>重特征值</mark>时，$\lambda_1=\lambda_2=\cdots=\lambda_r$，有$|\lambda_1|=|\lambda_2|=\cdots=|\lambda_r|\geq|\lambda_{r+1}|\geq\cdots\geq|\lambda_{n}|$。可知</p>
<div>$$
\begin{aligned}
v^{(m)}&amp;=k_1\lambda_1^mx_1+k_2\lambda_2^mx_2+\cdots+k_n\lambda_n^mx_n \\
&amp;=\lambda_1^m\left(\sum_{i=1}^rk_ix_i+\sum_{i=r+1}^n(\frac{\lambda_i}{\lambda_1})^mk_ix_i\right)
\end{aligned}
$$</div>
<p>同理，$v^{(m+1)}=\lambda_1^{m+1}\left(\sum_{i=1}^rk_ix_i+\sum_{i=r+1}^n(\frac{\lambda_i}{\lambda_1})^{m+1}k_ix_i\right)$。
取</p>
<div>$$
|(v^{(m)})_q|=\max_{1\leq i\leq n}(|(v^{(m)})_i|)
$$</div>
<p>则$\lambda_1\approx\frac{(v^{(m+1)})_q}{(v^{(m)})_q}$。
上述表明，当$A$的主特征值$\lambda_1$是重根时，幂法也能收敛到实际值。</p>
</li>

</ol>
</li>

</ul>
<h2>二、反幂法</h2>
<p>反幂法是求实方阵<mark>按模最小特征值</mark>的幂法。</p>
<ul>
<li>对于一个奇异矩阵，存在常数$\mu$，使得$A+\mu E$为非奇异矩阵。求$A+\mu E$的特征值$\widetilde{\lambda}_i,(i=1,2,\cdots,n)$，可以得到$\widetilde{\lambda}_i-\mu $即为矩阵$A$的特征值。</li>

</ul>
<p>设$A\in R^{n\times n}$为可逆矩阵，设$A$的特征值满足</p>
<div>$$
|\lambda_1|&gt;|\lambda_2|\geq\cdots\geq|\lambda_n|&gt;0
$$</div>
<p>则$A^{-1}$的特征值为$v_i=\frac{1}{\lambda_i}$，相应的特征向量$x_i$满足</p>
<div>$$
\frac{1}{|\lambda_1|}\leq\frac{1}{|\lambda_2|}\leq\cdots\leq\frac{1}{|\lambda_{n-1}|}&lt;\frac{1}{|\lambda_n|}
$$</div>
<p>因此计算$A$的<mark>按模最小</mark>特征值$\lambda_n$的问题，等价于计算$A^{-1}$的<mark>按模最大</mark>的特征值问题。对于$A^{-1}$使用乘幂法，可求得$A^{-1}$按模最大特征值$\frac{1}{\lambda_n}$，即求得了$A$的按模最小特征值$\lambda_n$。</p>
<h2>三、求$A$的指定特征值$\lambda_i$的幂法</h2>
<p>若$A$的特征值为$\lambda_1,\lambda_2,\cdots,\lambda_n$，特征向量为$x_1,x_2,\cdots,x_n$。$A-\alpha E$的特征值为$\lambda_1-\alpha,\lambda_2-\alpha,\cdots,\lambda_n-\alpha$，特征向量仍为$x_1,x_2,\cdots,x_n$。这里数$\alpha$形式上起到了<mark>使特征值向右移动</mark>的作用，称为<mark>原点位移因子</mark>。</p>
<p>如果提前知道矩阵$A$的某个特征值$\lambda_i$的估计值$\beta$。那么$\mu_i=\lambda_i-\beta$就是$A-\beta E$的最小特征值。按照反幂法就能把$\mu_i$求出来，然后$\mu_i+\beta$即为$\lambda_i$。</p>
</body>
</html>